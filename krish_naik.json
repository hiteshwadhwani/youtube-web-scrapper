{"url":{"0":"https:\/\/www.youtube.com\/watch?v=bDJkMOvhAmc"},"title":{"0":"Solve this Data Science Interview Question From Product Based Company"},"likes":{"0":"238"},"number_of_comments":{"0":"14"},"thumbnail":{"0":"http:\/\/img.youtube.com\/vi\/bDJkMOvhAmc\/maxresdefault.jpg"},"S3_Bucket_URL":{"0":"None"},"comments":{"0":[{"name":"Abhishek Chatterjee","comment":"In case of Kmeans it uses distance from centroid as a measure of similarity and due to this metrics it can compute distance of a unknown data point to the closest centroid and assign a label to it.\n\nFor hierarchical clustering algos, the problem statement is to figure out all possible sets of cluster which it does in a interative manner either starting from top \/ bottom and dividing \/ fusing data points using affinity and linkage method. So for a new data point it can not predict which cluster as it does not have a measure to do so other than recomputing entire clustering which is again the fit method.\nRead more"},{"name":"Souvik Adhikary","comment":"In my opinion The way of computing clusters makes all the difference:\n1. Firstly in K-means, a data-point is assigned to the cluster with the closest centroid and then the centroids are updated afterwards. So, predicting in the K-means is actually assigning the datapoints without updating the clusters, given that the prediction data are from the same distribution of data as the training set.\nRead more"},{"name":"Zevier Sunil Seema","comment":"I think it is because When you use k-means clustering you define the k i.e. the number of centroids i.e. also the number of clusters you want your k-means model to divide your dataset into. \nSo the number of clusters the k-means model divides the X_train dataset into is equal to the number of clusters the k-means model divides the X_test dataset into as the number of clusters\/centroids (k) is given by us to the model.\nRead more"},{"name":"Ayush Anchal","comment":"In k-means, new data point is assigned to the closest centroid \nBut in hierarchical clustering, it builds clusters iteratively starting from single data point,  if a new data point comes it will recompute the clusters an hence can entirely modify the final cluster."},{"name":"\ud835\udc79, \ud835\udc8b","comment":"Sir, which will be better to me in terms of Salary and opportunities? Software developer v\/s Data Scientist?\nI am third year Engineering student from tier-3 college..."},{"name":"Ayan Chowdhury","comment":"In my opinion, \nKmeans clustering uses number of clusters on the k value. It is just formation of non overlapping clusters which are similar to one another. Hence test data can be predicted. \nBut for agronomic clustering, it divides the data into bottom up or top down way. That is single cluster is created and natural hierarchy is formed as set of tree. On predicting test data the labels on which the tree or hierarchy can be formed in unknown. \nRead more"},{"name":"Nadmaan Fazeel","comment":"DBSCAN doesn't initialize centers because there is no centers in DBSCAN ( it works on connectivity of points). \nKmeans family perform classification( like KNN algorithm) using the previous cluster center and then update the centers. \nRead more"},{"name":"Harsh Maheshwari","comment":"for agglomerative it must be recomputational approach as top to down  which consume lots of work for algorithm........."},{"name":"Killer Gaming","comment":"Sir can you please tell a method in which data is provided in text files one by one like all the features have their name in rows and only one value for each feature is present in text file and 1000 of text files like this are there so how to merge all the text files in one csv file with features as columns"},{"name":"Harsh Maheshwari","comment":"in case of dbscan....it do it with density approach and single data point cannot be the estimator of density ...that's why prediction is not the case"},{"name":"Garv Soni","comment":"Main interview questions are do all these things without sklearn or other libraries do these thing with only python code "},{"name":"Jeet Pranav","comment":"\"Dibbe me dibba, dibbe me cake\nHamare Krish bhai laakho me ek \"\n "}]}}